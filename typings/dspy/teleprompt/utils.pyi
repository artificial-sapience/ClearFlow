from _typeshed import Incomplete
from dspy.teleprompt.bootstrap import BootstrapFewShot as BootstrapFewShot
from dspy.teleprompt.bootstrap import LabeledFewShot as LabeledFewShot

logger: Incomplete

def create_minibatch(trainset, batch_size: int = 50, rng=None): ...
def eval_candidate_program(batch_size, trainset, candidate_program, evaluate, rng=None): ...
def eval_candidate_program_with_pruning(trial, trial_logs, trainset, candidate_program, evaluate, trial_num, batch_size: int = 100): ...
def get_program_with_highest_avg_score(param_score_dict, fully_evaled_param_combos): ...
def calculate_last_n_proposed_quality(base_program, trial_logs, evaluate, trainset, devset, n): ...
def get_task_model_history_for_full_example(candidate_program, task_model, devset, evaluate): ...
def print_full_program(program) -> None: ...
def save_candidate_program(program, log_dir, trial_num, note=None): ...
def save_file_to_log_dir(source_file_path, log_dir) -> None: ...
def setup_logging(log_dir) -> None: ...
def get_token_usage(model) -> tuple[int, int]: ...
def log_token_usage(trial_logs, trial_num, model_dict) -> None: ...
def get_prompt_model(prompt_model): ...
def get_signature(predictor): ...
def set_signature(predictor, updated_signature) -> None: ...
def create_n_fewshot_demo_sets(student, num_candidate_sets, trainset, max_labeled_demos, max_bootstrapped_demos, metric, teacher_settings, max_errors=None, max_rounds: int = 1, labeled_sample: bool = True, min_num_samples: int = 1, metric_threshold=None, teacher=None, include_non_bootstrapped: bool = True, seed: int = 0, rng=None): ...
def old_getfile(object): ...
def new_getfile(object): ...
